{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPdGls2BFHZd",
        "outputId": "1fc515a6-d36c-4d39-be76-71f7091ec5c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading pymongo-4.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.7.0 pymongo-4.12.0\n",
            "Mounted at /content/drive\n",
            "Conexión exitosa: ['Trozas', 'Datasets', 'Clases', 'Detecciones', 'Entrenamientos', 'Imágenes', 'Plagas', 'Configuraciones', 'Plantaciones', 'Validaciones', 'Modelos', 'Usuarios']\n"
          ]
        }
      ],
      "source": [
        "# Instalamos las librerías necesarias\n",
        "!pip install pymongo tensorflow opencv-python matplotlib seaborn\n",
        "\n",
        "# Importamos las librerías\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from pymongo import MongoClient\n",
        "import io\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# Montamos Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Conexión a MongoDB Atlas\n",
        "MONGO_URI = \"mongodb+srv://ncoronado1006:Nelly2025@cluster0.txkst.mongodb.net/AgriZen?retryWrites=true&w=majority&appName=Cluster0\"  # Asegúrate de que la contraseña sea correcta\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client['AgriZen']\n",
        "\n",
        "# Definimos constantes globales\n",
        "IMAGE_SIZE = (256, 256)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Verificamos la conexión a MongoDB\n",
        "print(\"Conexión exitosa:\", db.list_collection_names())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directorio con imágenes en Google Drive\n",
        "img_dir = \"/content/drive/MyDrive/DATASET/data\"\n",
        "\n",
        "# Verificamos que el directorio exista\n",
        "if not os.path.exists(img_dir):\n",
        "    raise FileNotFoundError(f\"El directorio {img_dir} no existe. Asegúrate de que 'data' esté en DATASET.\")\n",
        "\n",
        "# Subir imágenes a MongoDB\n",
        "for class_name in ['Blight', 'Common_Rust', 'Gray_Leaf_Spot', 'Healthy']:\n",
        "    class_dir = os.path.join(img_dir, class_name)\n",
        "    if not os.path.exists(class_dir):\n",
        "        print(f\"Advertencia: Directorio {class_dir} no encontrado. Saltando esta clase.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Procesando clase: {class_name}\")\n",
        "    for img_file in os.listdir(class_dir):\n",
        "        img_path = os.path.join(class_dir, img_file)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            print(f\"Error al cargar {img_path}\")\n",
        "            continue\n",
        "        _, img_encoded = cv2.imencode('.jpg', img)\n",
        "        img_bytes = img_encoded.tobytes()\n",
        "\n",
        "        # Insertar imagen\n",
        "        imagen_id = db['Imágenes'].insert_one({\n",
        "            'ruta_imagen': img_bytes,\n",
        "            'fecha_subida': '2025-04-08',\n",
        "            'plantacion_id': 'plantacion_test',\n",
        "            'estado': 'procesada'\n",
        "        }).inserted_id\n",
        "\n",
        "        # Insertar plaga relacionada\n",
        "        # Guardamos el nombre sin guiones para que coincida con lo esperado después\n",
        "        plaga_name = class_name.replace('_', ' ')\n",
        "        plaga_id = db['Plagas'].insert_one({\n",
        "            'nombre': plaga_name,\n",
        "            'descripcion': f\"Plaga tipo {plaga_name}\",\n",
        "            'nivel_daño': 'Moderado',\n",
        "            'tratamiento_recomendado': 'Consultar experto'\n",
        "        }).inserted_id\n",
        "\n",
        "        # Insertar detección\n",
        "        db['Detecciones'].insert_one({\n",
        "            'imagen_id': imagen_id,\n",
        "            'plaga_id': plaga_id,\n",
        "            'confianza': 0.95,\n",
        "            'fecha_deteccion': '2025-04-08',\n",
        "            'estado': 'completado'\n",
        "        })\n",
        "\n",
        "print(\"Imágenes subidas a MongoDB con éxito.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK0jPqiWFmWf",
        "outputId": "d7e83705-c3c1-4f7a-bb91-b8e3c12e16c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando clase: Blight\n",
            "Procesando clase: Common_Rust\n",
            "Procesando clase: Gray_Leaf_Spot\n",
            "Procesando clase: Healthy\n",
            "Imágenes subidas a MongoDB con éxito.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para cargar imágenes y etiquetas desde MongoDB\n",
        "def load_data_from_mongo():\n",
        "    images = []\n",
        "    labels = []\n",
        "    collection = db['Imágenes']\n",
        "\n",
        "    for doc in collection.find():\n",
        "        img_data = doc['ruta_imagen']\n",
        "        img = Image.open(io.BytesIO(img_data))\n",
        "        img = img.resize(IMAGE_SIZE)\n",
        "        img_array = np.array(img)\n",
        "        images.append(img_array)\n",
        "\n",
        "        deteccion = db['Detecciones'].find_one({'imagen_id': doc['_id']})\n",
        "        if deteccion:\n",
        "            plaga = db['Plagas'].find_one({'_id': deteccion['plaga_id']})\n",
        "            labels.append(plaga['nombre'])\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Cargar los datos\n",
        "images, labels = load_data_from_mongo()\n",
        "class_names = sorted(list(set(labels)))\n",
        "label_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
        "labels = np.array([label_to_index[label] for label in labels])\n",
        "\n",
        "# Crear dataset de TensorFlow\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "dataset = dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Dividir en entrenamiento, validación y prueba\n",
        "def split_dataset(ds, train_split=0.8, val_split=0.1, test_split=0.1):\n",
        "    ds_size = len(list(ds))\n",
        "    if ds_size == 0:\n",
        "        raise ValueError(\"El dataset está vacío. Asegúrate de haber subido imágenes a MongoDB.\")\n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    train_ds = ds.take(train_size)\n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size + val_size)\n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "train_ds, val_ds, test_ds = split_dataset(dataset)\n",
        "\n",
        "print(f\"Clases detectadas: {class_names}\")\n",
        "print(f\"Tamaño del dataset de entrenamiento: {len(list(train_ds))}\")\n",
        "print(f\"Tamaño del dataset de validación: {len(list(val_ds))}\")\n",
        "print(f\"Tamaño del dataset de prueba: {len(list(test_ds))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcu8LwsBFv0f",
        "outputId": "74084bf4-eb59-474f-e4a7-215a75e6e3d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases detectadas: [np.str_('Blight'), np.str_('Common Rust'), np.str_('Gray Leaf Spot'), np.str_('Healthy')]\n",
            "Tamaño del dataset de entrenamiento: 157\n",
            "Tamaño del dataset de validación: 19\n",
            "Tamaño del dataset de prueba: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesamiento de imágenes\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "    layers.Resizing(IMAGE_SIZE[0], IMAGE_SIZE[1]),\n",
        "    layers.Rescaling(1.0/255)\n",
        "])\n",
        "\n",
        "# Definir el modelo\n",
        "n_classes = len(class_names)\n",
        "input_shape = (BATCH_SIZE, IMAGE_SIZE[0], IMAGE_SIZE[1], 3)\n",
        "model = models.Sequential([\n",
        "    resize_and_rescale,\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape[1:]),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(n_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callback para guardar el mejor modelo\n",
        "callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/content/best_model.h5',\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=70,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[callback]\n",
        ")\n",
        "\n",
        "# Guardar el modelo en MongoDB\n",
        "with open('/content/best_model.h5', 'rb') as f:\n",
        "    model_data = f.read()\n",
        "    db['Modelos'].insert_one({\n",
        "        'nombre': 'Modelo_Deteccion_Plagas',\n",
        "        'version': '1.0',\n",
        "        'ruta_modelo': model_data,\n",
        "        'fecha_entrenamiento': '2025-04-08',\n",
        "        'accuracy': float(history.history['accuracy'][-1])\n",
        "    })\n",
        "\n",
        "print(\"Modelo entrenado y guardado en MongoDB.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h08rzfxAF7S_",
        "outputId": "cc408cb5-5aed-4062-f1ef-c4aa587695ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.7473 - loss: 0.6376\n",
            "Epoch 1: val_loss improved from inf to 0.11061, saving model to /content/best_model.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 4s/step - accuracy: 0.7478 - loss: 0.6365 - val_accuracy: 0.9720 - val_loss: 0.1106\n",
            "Epoch 2/70\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.9454 - loss: 0.2286\n",
            "Epoch 2: val_loss improved from 0.11061 to 0.04320, saving model to /content/best_model.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 4s/step - accuracy: 0.9454 - loss: 0.2284 - val_accuracy: 0.9901 - val_loss: 0.0432\n",
            "Epoch 3/70\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.9697 - loss: 0.1364\n",
            "Epoch 3: val_loss did not improve from 0.04320\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m667s\u001b[0m 4s/step - accuracy: 0.9697 - loss: 0.1363 - val_accuracy: 0.9424 - val_loss: 0.2233\n",
            "Epoch 4/70\n",
            "\u001b[1m 95/157\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4:11\u001b[0m 4s/step - accuracy: 0.9552 - loss: 0.1707"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo\n",
        "model = tf.keras.models.load_model('/content/best_model.h5')\n",
        "\n",
        "# Evaluar en el dataset de prueba\n",
        "scores = model.evaluate(test_ds)\n",
        "print(f\"Pérdida en prueba: {scores[0]}, Precisión en prueba: {scores[1]}\")\n",
        "\n",
        "# Función de predicción\n",
        "def predict(model, img):\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = class_names[np.argmax(predictions[0])]\n",
        "    confidence = round(100 * np.max(predictions[0]), 2)\n",
        "    return predicted_class, confidence\n",
        "\n",
        "# Visualizar predicciones\n",
        "plt.figure(figsize=(15, 15))\n",
        "for images, labels in test_ds.take(1):\n",
        "    for i in range(min(12, len(images))):\n",
        "        ax = plt.subplot(3, 4, i+1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        pred_class, confidence = predict(model, images[i].numpy())\n",
        "        actual_class = class_names[labels[i]]\n",
        "        plt.title(f\"Real: {actual_class}\\nPred: {pred_class}\\nConf: {confidence}%\")\n",
        "        plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Matriz de confusión\n",
        "actual = []\n",
        "predicted = []\n",
        "for images, labels in test_ds:\n",
        "    for i in range(len(images)):\n",
        "        pred_class, _ = predict(model, images[i].numpy())\n",
        "        actual.append(class_names[labels[i]])\n",
        "        predicted.append(pred_class)\n",
        "\n",
        "cm = confusion_matrix(actual, predicted)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
        "plt.ylabel(\"Predicho\")\n",
        "plt.xlabel(\"Real\")\n",
        "plt.title(\"Matriz de Confusión\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6QmsFipL35c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YbKLiERo38U1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}